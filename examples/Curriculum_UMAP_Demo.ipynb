{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Curriculum Finetuning + UMAP Embedding Demo (Colab Friendly)\n",
        "\n",
        "This notebook demonstrates a compact, end-to-end workflow:\n",
        "1. Load the ESKAPE genomic features dataset.\n",
        "2. Prepare labels for curricular classification (using `contig_id`).\n",
        "3. Plot UMAP embeddings before and after a short finetuning run.\n",
        "\n",
        "The demo uses small samples to keep runtime reasonable on Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Installation\n",
        "\n",
        "If you are running in Google Colab, install the required packages below.\n",
        "If you are running locally from a checked-out repo where `prokbert` is already available, you can skip this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Core dependencies for the demo\n",
        "!pip -q install git+https://github.com/nbrg-ppcu/prokbert.git umap-learn seaborn datasets transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Seed\n",
        "\n",
        "We use a fixed seed for reproducibility, but the UMAP projection can still vary slightly across runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import ClassLabel, load_dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "from prokbert.curriculum_utils import plot_umap_embeddings\n",
        "from prokbert.models import ProkBertForCurricularClassification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Check (Colab)\n",
        "\n",
        "To enable GPU in Colab: `Runtime` -> `Change runtime type` -> `GPU`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Adjust these values to trade off speed vs. quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model_name = \"neuralbioinfo/prokbert-mini-long\"\n",
        "dataset_name = \"neuralbioinfo/ESKAPE-genomic-features\"\n",
        "dataset_split = \"ESKAPE\"\n",
        "output_dir = \"./curriculum_umap_demo\"\n",
        "\n",
        "seed = 42\n",
        "max_samples = 4000  # total samples for train/eval/test\n",
        "num_train_epochs = 1\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 16\n",
        "max_length = 256\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "These mirror the logic used in the Python example, with a focus on clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def resolve_columns(dataset):\n",
        "    sequence_candidates = (\"segment\", \"sequence\", \"seq\")\n",
        "    label_candidates = (\"contig_id\", \"class_label\", \"label\", \"labels\", \"y\")\n",
        "\n",
        "    sequence_col = next((c for c in sequence_candidates if c in dataset.column_names), None)\n",
        "    label_col = next((c for c in label_candidates if c in dataset.column_names), None)\n",
        "\n",
        "    if sequence_col is None:\n",
        "        raise ValueError(\"No sequence column found. Expected one of: segment, sequence, seq.\")\n",
        "    if label_col is None:\n",
        "        raise ValueError(\"No label column found. Expected one of: contig_id, class_label, label, labels, y.\")\n",
        "\n",
        "    return sequence_col, label_col\n",
        "\n",
        "\n",
        "def encode_labels(dataset, label_col):\n",
        "    if not isinstance(dataset.features[label_col], ClassLabel):\n",
        "        dataset = dataset.class_encode_column(label_col)\n",
        "\n",
        "    label_feature = dataset.features[label_col]\n",
        "    id2label = {i: name for i, name in enumerate(label_feature.names)}\n",
        "    label2id = {name: i for i, name in id2label.items()}\n",
        "    return dataset, id2label, label2id\n",
        "\n",
        "\n",
        "def tokenize_dataset(dataset, tokenizer, sequence_col, max_length):\n",
        "    def tokenize_function(examples):\n",
        "        tokenized = tokenizer(\n",
        "            examples[sequence_col],\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "        tokenized[\"labels\"] = examples[\"labels\"]\n",
        "        if \"sequence_id\" in examples:\n",
        "            tokenized[\"sequence_id\"] = examples[\"sequence_id\"]\n",
        "        return tokenized\n",
        "\n",
        "    remove_columns = [c for c in dataset.column_names if c not in (\"labels\", \"sequence_id\")]\n",
        "    num_proc = min(os.cpu_count() or 1, 8)\n",
        "    return dataset.map(tokenize_function, batched=True, remove_columns=remove_columns, num_proc=num_proc)\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits = eval_pred.predictions if hasattr(eval_pred, \"predictions\") else eval_pred[0]\n",
        "    labels = eval_pred.label_ids if hasattr(eval_pred, \"label_ids\") else eval_pred[1]\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, preds)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Prepare the Dataset\n",
        "\n",
        "We keep only the sequence and label columns, then add a `sequence_id` for UMAP grouping.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "dataset = load_dataset(dataset_name, split=dataset_split)\n",
        "dataset = dataset.shuffle(seed=seed)\n",
        "\n",
        "if max_samples:\n",
        "    dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
        "\n",
        "sequence_col, label_col = resolve_columns(dataset)\n",
        "\n",
        "dataset = dataset.remove_columns(\n",
        "    [col for col in dataset.column_names if col not in (sequence_col, label_col)]\n",
        ")\n",
        "\n",
        "dataset = dataset.add_column(\"sequence_id\", list(range(len(dataset))))\n",
        "\n",
        "dataset, id2label, label2id = encode_labels(dataset, label_col)\n",
        "if label_col != \"labels\":\n",
        "    dataset = dataset.rename_column(label_col, \"labels\")\n",
        "\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Eval/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "split = dataset.train_test_split(test_size=0.2, seed=seed)\n",
        "temp = split[\"test\"].train_test_split(test_size=0.5, seed=seed)\n",
        "train_ds = split[\"train\"]\n",
        "eval_ds = temp[\"train\"]\n",
        "test_ds = temp[\"test\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "train_ds = tokenize_dataset(train_ds, tokenizer, sequence_col, max_length)\n",
        "eval_ds = tokenize_dataset(eval_ds, tokenizer, sequence_col, max_length)\n",
        "test_ds = tokenize_dataset(test_ds, tokenizer, sequence_col, max_length)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Setup and UMAP (Before Training)\n",
        "\n",
        "We plot embeddings from a shuffled sample of the training set (up to 1,000 examples).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "model_dtype = torch.bfloat16 if use_bf16 else torch.float32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ProkBertForCurricularClassification.from_pretrained(\n",
        "    model_name,\n",
        "    curricular_num_labels=len(id2label),\n",
        "    curricular_face_m=0.5,\n",
        "    curricular_face_s=64.0,\n",
        "    classification_dropout_rate=0.1,\n",
        "    curriculum_hidden_size=128,\n",
        "    torch_dtype=model_dtype,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "plot_umap_embeddings(\n",
        "    model,\n",
        "    train_ds,\n",
        "    data_collator,\n",
        "    output_dir,\n",
        "    \"umap_before_training.png\",\n",
        "    eval_batch_size,\n",
        "    seed,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "We run a short finetuning pass to keep this demo fast.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"none\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=25,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=eval_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    load_best_model_at_end=False,\n",
        "    bf16=use_bf16,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## UMAP (After Training)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plot_umap_embeddings(\n",
        "    model,\n",
        "    train_ds,\n",
        "    data_collator,\n",
        "    output_dir,\n",
        "    \"umap_after_training.png\",\n",
        "    eval_batch_size,\n",
        "    seed,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "print(\"Before training\")\n",
        "display(Image(filename=os.path.join(output_dir, \"umap_before_training.png\")))\n",
        "\n",
        "print(\"After training\")\n",
        "display(Image(filename=os.path.join(output_dir, \"umap_after_training.png\")))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}