{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b700d0-ff0e-4940-a2cc-8fbfe9873660",
   "metadata": {},
   "source": [
    "# Inference and Evaluation with the Finetuned Models\n",
    "\n",
    "In this notebook, we demonstrate how one can evaluate various finetuned models on both the promoter and phage test datasets.\n",
    "\n",
    "The main steps are:\n",
    "  * Preparing the models and datasets\n",
    "  * Setting up the parameters for the evaluation\n",
    "  * Running inference and collecting the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77070a04-3f9c-4183-bbbf-832407b376e5",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "While ProkBERT can operate on CPUs, leveraging GPUs significantly accelerates the process. Google Colab offers free GPU usage (subject to time and memory limits), making it an ideal platform for trying and experimenting with ProkBERT models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffda82f-ddb0-4107-8ecb-0d9d2cd4bfc8",
   "metadata": {},
   "source": [
    "### Enabling and testing the GPU (if you are using google colab)\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa017b-a043-4e54-8e0e-33638192f137",
   "metadata": {},
   "source": [
    "### Setting up the packages and the installs\n",
    "First, we'll install the ProkBERT package directly from its GitHub repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fb310-003a-4f05-be02-b567fd0b62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, roc_auc_score, recall_score\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cba33b-d853-4eb6-86c0-b9beeb28598e",
   "metadata": {},
   "source": [
    "Next, we'll confirm that we can connect to the GPU with pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfa647-993f-4c57-9dcc-38115b962ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError('GPU device not found')\n",
    "else:\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f'Found GPU at: {device_name}')\n",
    "num_cores = os.cpu_count() \n",
    "print(f'Number of available CPU cores: {num_cores}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea0dc2-73ea-414e-a1d7-06a14ae3abbf",
   "metadata": {},
   "source": [
    "## Loading the Finetuned Model and the tokenizer for Promoter Identification\n",
    "\n",
    "Next, we will download the finetuned model for promoter identification. For more details about the model, please see the Finetuning notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb747085-7895-466f-bdd6-d2f010bd58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = \"neuralbioinfo/prokbert-mini-promoter\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_model, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(finetuned_model, trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a53937-5ddc-4b90-80d9-0839b4ea7ff1",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "In this section, we will evaluate the test sets of the promoter dataset. We have two different types of tests: a sigma70 test (known E. coli promoters) referred to as the 'test_sigma70' set, and a multispecies dataset, which consists of promoters from various species as well as CDS sequences (non-promoters) and randomly generated sequences. For a more detailed description, see: [Bacterial Promoters Dataset](https://huggingface.co/datasets/neuralbioinfo/bacterial_promoters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def9796-2c57-4813-98fc-12bbff3ff2aa",
   "metadata": {},
   "source": [
    "### Tokenization of Dataset\n",
    "\n",
    "The following code defines a function to tokenize the input sequences and applies it to a prokaryotic promoter dataset. \n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "1. **`tokenize_function`**:\n",
    "   - Tokenizes the nucleotide sequences found in the `segment` field of the dataset.\n",
    "   - Utilizes `batch_encode_plus` from the tokenizer to:\n",
    "     - Add padding for uniform sequence lengths.\n",
    "     - Include special tokens required by the model.\n",
    "     - Return the encoded output as PyTorch tensors (`return_tensors=\"pt\"`).\n",
    "\n",
    "2. **Dataset Loading**:\n",
    "   - The dataset is sourced from Hugging Face's `neuralbioinfo/bacterial_promoters` dataset, specifically using the `test_sigma70` split.\n",
    "\n",
    "3. **Tokenization Mapping**:\n",
    "   - The `tokenize_function` is applied to the dataset in a **batched** manner.\n",
    "   - The `num_proc` parameter allows the use of multiple CPU cores to speed up the tokenization process.\n",
    "\n",
    "This pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98f2dd-48b0-4daf-96b2-a8d9f6eca397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the input sequences\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        examples[\"segment\"],\n",
    "        padding=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # Return the updated dictionary\n",
    "    return encoded\n",
    "    \n",
    "dataset = load_dataset(\"neuralbioinfo/bacterial_promoters\", split='test_sigma70')\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=num_cores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb385f-9b83-4400-b2f4-4707fe250967",
   "metadata": {},
   "source": [
    "### Performing Inference with ProkBERT\n",
    "\n",
    "The following code demonstrates how to perform inference using a pretrained ProkBERT model with the Hugging Face `Trainer` API.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "1. **Defining `TrainingArguments`**:\n",
    "   - **`output_dir`**: Specifies the directory where results and logs will be stored.\n",
    "   - **`per_device_eval_batch_size`**: Sets the batch size for inference to 128, ensuring efficient processing.\n",
    "   - **`logging_dir`**: Directory for logging during evaluation.\n",
    "   - **`report_to`**: Disables logging to external services like Weights & Biases (W&B).\n",
    "\n",
    "2. **Initializing the Trainer**:\n",
    "   - The `Trainer` is initialized with:\n",
    "     - **`model`**: The pretrained ProkBERT model used for inference.\n",
    "     - **`args`**: Inference-specific arguments defined in `training_args`.\n",
    "\n",
    "3. **Performing Inference**:\n",
    "   - The `predict` method of the `Trainer` is used to perform inference on the `tokenized_dataset`.\n",
    "   - The output, stored in `predictions`, contains the model's predictions for the input sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba7ba0-0efa-4ecc-b74b-cfabbdffad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define TrainingArguments for inference\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              # Directory for storing logs/results\n",
    "    per_device_eval_batch_size=128,     # Batch size for inference\n",
    "    logging_dir=\"./logs\",               # Directory for logs\n",
    "    report_to=\"none\",                   # Disable reporting to W&B or other loggers\n",
    ")\n",
    "\n",
    "# Initialize Trainer for inference\n",
    "trainer = Trainer(\n",
    "    model=model,                        # Pretrained ProkBERT model\n",
    "    args=training_args,                 # Inference arguments\n",
    ")\n",
    "\n",
    "# Perform inference\n",
    "predictions = trainer.predict(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242022c1-3f13-4e56-94b8-d74ba61f0311",
   "metadata": {},
   "source": [
    "### Processing Predictions and Creating Final DataFrame\n",
    "\n",
    "This code processes model predictions to generate class probabilities, predicted labels, and a final DataFrame for analysis.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Compute Probabilities**:\n",
    "   - Apply `softmax` to model outputs to calculate probabilities for each class (`prob_class_0` and `prob_class_1`).\n",
    "\n",
    "2. **Predict Classes**:\n",
    "   - Use `np.argmax` to determine the predicted class (`predicted_y`) with the highest probability.\n",
    "\n",
    "3. **Prepare DataFrame**:\n",
    "   - Convert the dataset to a Pandas DataFrame.\n",
    "   - Drop unnecessary columns (`segment`, `Strand`, `ppd_original_SpeciesName`).\n",
    "\n",
    "4. **Add Predictions**:\n",
    "   - Add columns for class probabilities, predicted class, and human-readable labels (`promoter` or `non_promoter`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94a12e-0b4d-4a89-afe5-1f588e757f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process predictions\n",
    "probabilities = softmax(predictions.predictions, axis=1)\n",
    "predicted_y = np.argmax(probabilities, axis=1)\n",
    "dataset_df = dataset.to_pandas()\n",
    "dataset_df.drop(columns=[\"segment\", \"Strand\", \"ppd_original_SpeciesName\"], inplace=True)\n",
    "dataset_df[\"prob_class_0\"] = probabilities[:, 0]\n",
    "dataset_df[\"prob_class_1\"] = probabilities[:, 1]\n",
    "dataset_df[\"predicted_y\"] = predicted_y\n",
    "\n",
    "# Add 'promoter' or 'non_promoter' label\n",
    "dataset_df[\"predicted_label\"] = dataset_df[\"predicted_y\"].apply(lambda x: \"promoter\" if x == 1 else \"non_promoter\")\n",
    "\n",
    "# Display final dataframe\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a8a20-75b5-4a5e-89fc-21302714f6a0",
   "metadata": {},
   "source": [
    "# Evaluating the promoter models prediction performance\n",
    "\n",
    "This code evaluates the ProkBERT model on a labeled dataset using Hugging Face's `Trainer` and evaluation utilities from the ProkBERT package.\n",
    "\n",
    "**Install ProkBERT**:\n",
    "   - Install the ProkBERT package: `!pip install prokbert`.\n",
    "   - Import the `compute_metrics_eval_prediction` function for computing evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf102b-d774-45f8-a180-1a4a4e122989",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prokbert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdcd9e4-0b03-445f-9e67-89bdd01e522a",
   "metadata": {},
   "source": [
    "### Tokenizing the Evaluation Dataset\n",
    "\n",
    "This code defines a `tokenize_function` to preprocess the evaluation dataset for ProkBERT. It tokenizes the `segment` sequences, adds padding and special tokens, and adjusts the `attention_mask` to exclude masked tokens (IDs `2` and `3`). The labels (`y`) are converted into PyTorch tensors and included in the output. The function is applied to the entire dataset in batches using parallel processing to create a tokenized dataset with labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce26720-62b0-45c4-bc35-9d7201fb26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the evaluation dataset\n",
    "def tokenize_function(examples):\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        examples[\"segment\"],\n",
    "        padding=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = encoded[\"input_ids\"].clone().detach()\n",
    "    attention_mask = encoded[\"attention_mask\"].clone().detach()\n",
    "    mask_tokens = (input_ids == 2) | (input_ids == 3)\n",
    "    attention_mask[mask_tokens] = 0\n",
    "    y = torch.tensor(examples[\"y\"], dtype=torch.int64)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": y,\n",
    "    }\n",
    "\n",
    "tokenized_dataset_with_labels = dataset.map(tokenize_function, batched=True, num_proc=num_cores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e3593-e4df-41d7-8390-a31a03af556a",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "The `Trainer` is initialized for evaluation with the ProkBERT model, using the tokenized dataset and predefined evaluation arguments. The `compute_metrics_eval_prediction` function calculates performance metrics such as accuracy, precision, recall, and F1 score. The evaluation is performed using the `evaluate` method, and the results are stored in `evaluation_results`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40c823-f40d-4824-9fe7-9beae5cb2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer for evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_dataset_with_labels,\n",
    "    compute_metrics=compute_metrics_eval_prediction,\n",
    ")\n",
    "\n",
    "# Perform evaluation\n",
    "print(\"Starting evaluation...\")\n",
    "evaluation_results = trainer.evaluate()\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79627d-a3a0-4b06-adbb-e7f38501aa81",
   "metadata": {},
   "source": [
    "### Final Remarks\n",
    "Stay curious, stay caffeinated, and happy coding! ðŸ˜ŽðŸ’»\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
