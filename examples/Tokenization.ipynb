{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d169ce5-e121-48c4-a97d-06d6ce1a03b8",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "This notebooks will present the tokenization of sequence for prokbert.\n",
    "Parts are:\n",
    "  * Tokenization process background and parameters\n",
    "  * Tokenization of sequences\n",
    "  * Tokenization for pretraining\n",
    "  * HDF datasets for storing preprocessed sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d384f0-2b14-48da-a9a9-ed7d76b427c0",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenization of Sequence Data\n",
    "\n",
    "ProkBERT employs LCA tokenization, leveraging overlapping k-mers to capture rich local context information, enhancing model generalization and performance. The key parameters are the k-mer size and shift. For instance, with a k-mer size of 6 and a shift of 1, the tokenization captures detailed sequence information, while a k-mer size of 1 represents a basic character-based approach.\n",
    "\n",
    "### Segmentation Strategies\n",
    "Before tokenization, sequences are segmented using two main approaches:\n",
    "1. **Contiguous Sampling**: Divides contigs into non-overlapping segments.\n",
    "2. **Random Sampling**: Fragments the input sequence into randomly sized segments.\n",
    "\n",
    "### Tokenization Process\n",
    "After segmentation, sequences are encoded into a simpler vector format. The LCA method is pivotal in this phase, allowing the model to use a broader context and reducing computational demands while maintaining the information-rich local context.\n",
    "\n",
    "### Context Size Limitations\n",
    "It's important to note that transformer models, including ProkBERT, have a context size limitation. ProkBERT's design accommodates context sizes significantly larger than an average gene, yet smaller than the average bacterial genome.\n",
    "\n",
    "We provide pretrained models for variants like ProkBERT-mini (k-mer size 6, shift 1), ProkBERT-mini-c (k-mer size 1, shift 1), and ProkBERT-mini-long (k-mer size 6, shift 2), catering to different sequence analysis requirements.\n",
    "\n",
    "<img src=\"https://github.com/nbrg-ppcu/prokbert/blob/main/assets/Figure2_tokenization.png?raw=true\" width=\"800\" alt=\"Segmentation Process\"> \n",
    "\n",
    "*Figure: The tokenization process in ProkBERT.*\n",
    "\n",
    "It is important to see that, when the $shift > 1$ there are multiple possible tokenization, depending where we start the tokenizaiton. The window offset refers the actual tokenization window. \n",
    "I.e. let the sequence be `ATGTCCGCGACCTTTCATACATACCACCGGTAC` with the k-mer size 6, shift 2) we will have two possible tokenization:\n",
    "\n",
    "Tokenization with offset=0:\n",
    "```plaintext\n",
    "    ATGTCCGCGACCTTTCATACATACCACCGGTAC\n",
    "0.  ATGTCC  GACCTT  ATACAT  CACCGG\n",
    "1.    GTCCGC  CCTTTC  ACATAC  CCGGTA\n",
    "2.      CCGCGA  TTTCAT  ATACCA\n",
    "3.        GCGACC  TCATAC  ACCACC\n",
    "```\n",
    "Tokenization with offset=1\n",
    "```plaintext\n",
    "    ATGTCCGCGACCTTTCATACATACCACCGGTAC\n",
    "0.   TGTCCG  ACCTTT  TACATA  ACCGGT\n",
    "1.     TCCGCG  CTTTCA  CATACC  CGGTAC\n",
    "2.       CGCGAC  TTCATA  TACCAC\n",
    "3.         CGACCT  CATACA  CCACCG\n",
    "```\n",
    "By default all possible tokenization is returned. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4bad4-cfcc-4f6e-8ae2-6b2cf15f3dea",
   "metadata": {},
   "source": [
    "## Key Tokenization Parameters\n",
    "\n",
    "The most important tokenization parameters are the **k-mer size** and **shift**. \n",
    "\n",
    "The autotokenizer takes care of these for you, matching the right settings to the model. Easy and hassle-free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bdb3ef-a97b-4ef4-9800-b12e93e45cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the autotokenizer for the ProkBERT-mini model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralbioinfo/prokbert-mini\", trust_remote_code=True)\n",
    "\n",
    "# Sample sequence\n",
    "sequence = \"ATGTCCGCGACCTTTCATACATACCACCGGTAC\"\n",
    "\n",
    "# Tokenize the sequence\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "# Encode the sequence to get token IDs\n",
    "encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "token_ids = encoded[\"input_ids\"]\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4088f8-4d06-4791-9af2-82b5e17c4bda",
   "metadata": {},
   "source": [
    "### Converting Token IDs Back to Sequences\n",
    "Decode token IDs to view the kmer sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97245b1-f1f0-4cd4-a82e-4d992c9b029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer(\"ATGTCCGCGACCTT\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "decoded_sequence = tokenizer.decode(token_ids[0])\n",
    "print(decoded_sequence)  # Output: [CLS] ATGTCCGCGACCTT [SEP]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3844692-d27b-4752-acee-e53daef4c9d0",
   "metadata": {},
   "source": [
    "## Processing Batches of Sequences\n",
    "\n",
    "Keep in mind: the tokenizer doesnâ€™t clean or preprocess your data. It assumes your sequences are ready to goâ€”uppercase, properly chunked, and strictly nucleotide sequences. If your data isnâ€™t quite there yet, you can use the **segmentation process** (see the [segmentation notebook](https://github.com/nbrg-ppcu/prokbert/blob/main/examples/Segmentation.ipynb)) and the handy **sequtils** in ProkBERT, which are built to handle large corpora of sequence data.\n",
    "\n",
    "Already have a dataset in a Pandas DataFrame or a Hugging Face Dataset? Just define a `tokenize_function` and run the tokenization process. \n",
    "\n",
    "- **For training**: Make sure you prepare `input_ids`, `attention_mask`, and `labels`.\n",
    "- **For inference**: You only need `input_ids` and `attention_mask`â€”no labels required.\n",
    "\n",
    "If youâ€™re working with large datasets, Hugging Faceâ€™s dataset utilities make it efficient to tokenize on the fly during training. \n",
    "\n",
    "For more details:\n",
    "- Check out the **inference notebook** for inference examples.\n",
    "- Dive into the **fine-tuning notebook** if you're preparing for model training.\n",
    "\n",
    "Tokenizing big sequence corpora doesnâ€™t have to be a headache! ðŸ˜‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74224bd1-ce83-4945-9661-9c9542424ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets just install the Huggingface datasets for the examples\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94a9c3-afb4-449e-b111-783171fb30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralbioinfo/prokbert-mini\", trust_remote_code=True)\n",
    "data = {\"segment\": [\"ATGTCCGCGACCTT\", \"TGCATACCAGTCCG\"]}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"segment\"], padding=True, truncation=True)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf8700-ce5d-4e0a-90d9-9da8f3297451",
   "metadata": {},
   "source": [
    "### Batch Tokenization with Labels for Training\n",
    "\n",
    "Prepare batches of sequences with labels for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d5c1b-fadc-49d4-8a1e-2fa200ff16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_labels(examples):\n",
    "    encoded = tokenizer(examples[\"segment\"], padding=True, truncation=True)\n",
    "    encoded[\"labels\"] = examples[\"y\"]\n",
    "    return encoded\n",
    "\n",
    "# Example dataset\n",
    "data = {\"segment\": [\"ATGTCC\", \"TGCATC\"], \"y\": [1, 0]}\n",
    "dataset = Dataset.from_dict(data)\n",
    "tokenized_dataset = dataset.map(tokenize_with_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de3036-c77b-439e-876e-885d3c88a877",
   "metadata": {},
   "source": [
    "### Tokenization Function for Training\n",
    "\n",
    "The `tokenize_function` prepares sequence data for ProkBERT training by encoding input sequences, handling special token masks, and attaching labels.\n",
    "\n",
    "#### Key Steps:\n",
    "- **Tokenization**: Uses `batch_encode_plus` to tokenize the `segment` field, adding padding and special tokens. The resulting `input_ids` and `attention_mask` tensors are detached for further processing.\n",
    "- **Masking Special Tokens**: Updates the `attention_mask` to ignore tokens with IDs `2` and `3` (e.g., padding or special tokens) for better training efficiency.\n",
    "- **Label Preparation**: Converts the `y` column (class labels) into a PyTorch tensor, labeled as `labels`.\n",
    "\n",
    "#### Notes:\n",
    "- If segment lengths vary significantly, consider using a `data_collator` (e.g., Hugging Face's `DataCollatorForTokenClassification`) to handle padding and batching dynamically during training.\n",
    "\n",
    "This function ensures clean and efficient data preparation for training while handling masking and label integration seamlessly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af7d91-34c3-45e3-bdaf-5d51c4cce22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer for ProkBERT-mini\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralbioinfo/prokbert-mini\", trust_remote_code=True)\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    \"segment\": [\n",
    "        \"ATGTCCGCGACCTT\",\n",
    "        \"TGCATACCAGTCCG\",\n",
    "        \"ATGCC\",\n",
    "        \"GCGTACCAG\",\n",
    "    ],\n",
    "    \"y\": [1, 0, 1, 0]\n",
    "}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize and preprocess the input sequences\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        examples[\"segment\"],\n",
    "        add_special_tokens=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Clone and modify input_ids and attention_mask for masking special tokens\n",
    "    input_ids = encoded[\"input_ids\"].clone().detach()\n",
    "    attention_mask = encoded[\"attention_mask\"].clone().detach()\n",
    "    mask_tokens = (input_ids == 2) | (input_ids == 3)\n",
    "    attention_mask[mask_tokens] = 0\n",
    "\n",
    "    # Add labels\n",
    "    labels = torch.tensor(examples[\"y\"], dtype=torch.int64)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa8dd1-2a42-445a-8d96-11e4c8ae069c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8dc4d-5e31-49d7-9f85-db5f8f04cccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1b368-ee4f-47d5-8016-5bd1d5e8593b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130a04fa-a0c6-47a1-a1f1-dd045aced74f",
   "metadata": {},
   "source": [
    "# The old solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c345ea8-d8a0-4d57-90c9-7d36bc9dabd9",
   "metadata": {},
   "source": [
    "## Tokenization parameters\n",
    "\n",
    "The following table outlines the configuration parameters for ProkBERT, detailing their purpose, default values, types, and constraints.\n",
    "\n",
    "\n",
    "| Parameter | Description | Type | Default | Constraints |\n",
    "|-----------|-------------|------|---------|-------------|\n",
    "| **Tokenization** |\n",
    "| `type` | Describes the tokenization approach. By default, the LCA (Local Context Aware) method is used. | string | `lca` | Options: `lca` |\n",
    "| `kmer` | Determines the k-mer size for the tokenization process. | integer | 6 | Options: 1-9 |\n",
    "| `shift` | Represents the shift parameter in k-mer. The default value is 1. | integer | 1 | Min: 0 |\n",
    "| `max_segment_length` | Gives the maximum number of characters in a segment. This should be consistent with the language model's capability. It can be alternated with token_limit. | integer | 2050 | Min: 6, Max: 4294967296 |\n",
    "| `token_limit` | States the maximum token count that the language model can process, inclusive of special tokens like CLS and SEP. This is interchangeable with max_segment_length. | integer | 4096 | Min: 1, Max: 4294967296 |\n",
    "| `max_unknown_token_proportion` | Defines the maximum allowed proportion of unknown tokens in a sequence. For instance, if 10% of the tokens are unknown (when max_unknown_token_proportion=0.1), the segment won't be tokenized. | float | 0.9999 | Min: 0, Max: 1 |\n",
    "| `vocabfile` | Path to the vocabulary file. If set to 'auto', the default vocabulary is utilized. | str | `auto` | - |\n",
    "| `vocabmap` | The default vocabmap loaded from file | dict | `{}` | - |\n",
    "| `isPaddingToMaxLength` | Determines if the tokenized sentence should be padded with [PAD] tokens to produce vectors of a fixed length. | bool | False | Options: True, False |\n",
    "| `add_special_token` | The tokenizer should add the special starting and sentence end tokens. The default is yes. | bool | True | Options: True, False |\n",
    "| **Computation** |\n",
    "| `cpu_cores_for_segmentation` | Specifies the number of CPU cores allocated for the segmentation process. | integer | 10 | Min: 1 |\n",
    "| `cpu_cores_for_tokenization` | Allocates a certain number of CPU cores for the k-mer tokenization process. | integer | -1 | Min: 1 |\n",
    "| `batch_size_tokenization` | Determines the number of segments a single core processes at a time. The input segment list will be divided into chunks of this size. | integer | 10000 | Min: 1 |\n",
    "| `batch_size_fasta_segmentation` | Sets the number of fasta files processed in a single batch, useful when dealing with a large number of fasta files. | integer | 3 | Min: 1 |\n",
    "| `numpy_token_integer_prec_byte` | The type of integer to be used during the vectorization. The default is 2, if you want to work larger k-mers then increase it to 4. 1: np.int8, 2:np.int16. 4:np.int32. 8: np.int64 | integer | 2 | Options: 1, 2, 4, 8 |\n",
    "| `np_tokentype` | Dummy | type | `np.int16` | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576a136-a8e7-4d4c-af14-2264b8995e6f",
   "metadata": {},
   "source": [
    "## Installation of ProkBERT (if needed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66218845-48e1-4e3e-af3c-b4e31411e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import prokbert\n",
    "    print(\"ProkBERT is already installed.\")\n",
    "except ImportError:\n",
    "    !pip install prokbert\n",
    "    print(\"Installed ProkBERT.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488de6fa-248a-47fa-be82-f4597c62951e",
   "metadata": {},
   "source": [
    "# Tokenization of sequences examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72e9bf-f782-4cc2-8fc3-efefab5e2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prokbert.config_utils import *\n",
    "from prokbert.sequtils import *\n",
    "tokenization_parameters = {'kmer' : 6,\n",
    "                          'shift' : 2}\n",
    "\n",
    "segment = 'ATGTCCGCGACCT'\n",
    "defconfig = SeqConfig() # For the detailed configarion parameters see the table above\n",
    "tokenization_params = defconfig.get_and_set_tokenization_parameters(tokenization_parameters)\n",
    "tokens, kmers = lca_tokenize_segment(segment, tokenization_params)\n",
    "\n",
    "print(' '.join([str(t) for t in tokens]))\n",
    "print(' '.join(kmers[0]))\n",
    "\n",
    "results_pretty_print = pretty_print_overlapping_sequence(segment, kmers[0], tokenization_params)\n",
    "print(results_pretty_print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f35c62-1a37-474c-a7df-2508cccc7350",
   "metadata": {},
   "source": [
    "## Tokenizing Longer Sequences\n",
    "\n",
    "To tokenize sequences longer than what the current ProkBERT model supports, you can adjust the `token_limit` and `max_segment_length` parameters. Keep in mind that the tokenization process is parallelized using Python's multiprocessing module at the segment level. Therefore, it's important to also consider adjusting the number of cores utilized, as well as the `batch_size_tokenization` parameter, which determines how many sequences a core should process at once. Failing to appropriately adjust these settings might lead to memory issues.\n",
    "\n",
    "\n",
    "### Example Python Code for Long Sequence Tokenization\n",
    "\n",
    "Below is an example of how you can configure and use the tokenization for longer sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fdb064-3e15-4422-bc86-fe8424a11c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prokbert.sequtils import lca_tokenize_segment\n",
    "from prokbert.config_utils import SeqConfig\n",
    "\n",
    "# Tokenization parameters\n",
    "tokenization_parameters = {\n",
    "    'kmer': 6,\n",
    "    'shift': 1,\n",
    "    'max_segment_length': 2000000,\n",
    "    'token_limit': 2000000\n",
    "}\n",
    "\n",
    "# Example of a long sequence\n",
    "segment = 'ATGTCCGCGACCT' * 100000\n",
    "\n",
    "# Default configuration for tokenization\n",
    "defconfig = SeqConfig() # For detailed configuration parameters, refer to the table above\n",
    "\n",
    "# Get and set tokenization parameters\n",
    "tokenization_params = defconfig.get_and_set_tokenization_parameters(tokenization_parameters)\n",
    "\n",
    "# Perform tokenization\n",
    "tokens, kmers = lca_tokenize_segment(segment, tokenization_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3a888-53db-4188-b4de-16e09752fce2",
   "metadata": {},
   "source": [
    "# Tokenization for Pretraining\n",
    "\n",
    "Given the abundance and large size of sequence data, preprocessing this data in advance and storing it for later use is a recommended practice. The primary steps in this process are segmentation and tokenization.\n",
    "\n",
    "The outcome of tokenization is a set of vectors, which need to be converted into a matrix-like structure, typically through padding. Additionally, randomizing these vectors is essential for effective training. The `sequtils` module in ProkBERT includes utilities to facilitate these steps. Below, we outline some examples of how to accomplish this.\n",
    "\n",
    "## Basic Steps for Preprocessing:\n",
    "\n",
    "1. **Load Fasta Files**: Begin by loading the raw sequence data from FASTA files.\n",
    "2. **Segment the Raw Sequences**: Apply segmentation parameters to split the sequences into manageable segments.\n",
    "3. **Tokenize the Segmented Database**: Use the defined tokenization parameters to convert the segments into tokenized forms.\n",
    "4. **Create a Padded/Truncated Array**: Generate a uniform array structure, padding or truncating as necessary.\n",
    "5. **Save the Array to HDF**: Store the processed data in an HDF (Hierarchical Data Format) file for efficient retrieval and use in training models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918af92-82a0-4072-9437-4096a316aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from os.path import join\n",
    "from prokbert.sequtils import *\n",
    "\n",
    "# Directory for pretraining FASTA files\n",
    "pretraining_fasta_files_dir = pkg_resources.resource_filename('prokbert','data/pretraining')\n",
    "\n",
    "# Define segmentation and tokenization parameters\n",
    "segmentation_params = {\n",
    "    'max_length': 256,  # Split the sequence into segments of length L\n",
    "    'min_length': 6,\n",
    "    'type': 'random'\n",
    "}\n",
    "tokenization_parameters = {\n",
    "    'kmer': 6,\n",
    "    'shift': 1,\n",
    "    'max_segment_length': 2003,\n",
    "    'token_limit': 2000\n",
    "}\n",
    "\n",
    "# Setup configuration\n",
    "defconfig = SeqConfig()\n",
    "segmentation_params = defconfig.get_and_set_segmentation_parameters(segmentation_params)\n",
    "tokenization_params = defconfig.get_and_set_tokenization_parameters(tokenization_parameters)\n",
    "\n",
    "# Load and segment sequences\n",
    "input_fasta_files = [join(pretraining_fasta_files_dir, file) for file in get_non_empty_files(pretraining_fasta_files_dir)]\n",
    "sequences = load_contigs(input_fasta_files, IsAddHeader=True, adding_reverse_complement=True, AsDataFrame=True, to_uppercase=True, is_add_sequence_id=True)\n",
    "segment_db = segment_sequences(sequences, segmentation_params, AsDataFrame=True)\n",
    "\n",
    "# Tokenization\n",
    "tokenized = batch_tokenize_segments_with_ids(segment_db, tokenization_params)\n",
    "expected_max_token = max(len(arr) for arrays in tokenized.values() for arr in arrays)\n",
    "X, torchdb = get_rectangular_array_from_tokenized_dataset(tokenized, tokenization_params['shift'], expected_max_token)\n",
    "\n",
    "# Save to HDF file\n",
    "hdf_file = '/tmp/pretraining.h5'\n",
    "save_to_hdf(X, hdf_file, database=torchdb, compression=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e07efc-f14e-4164-bf50-3ba036d2065d",
   "metadata": {},
   "source": [
    "### Tokenization with tokenizer class\n",
    "\n",
    "The tokenizer class can be used for tokenization as well. There are various additional features as well. The tokenizer might operate on the original sequence space or k-mer space, the default is the latter. \n",
    "For example how to encode and decode sequence is important and we will give you examples here. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
