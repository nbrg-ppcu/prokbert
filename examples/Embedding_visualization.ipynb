{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3d2a41-506d-48a0-aac7-2b0ef7f3f680",
   "metadata": {},
   "source": [
    "# Sequence Representation Visualization with ProkBERT\n",
    "\n",
    "This guide outlines the steps to visualize sequence embeddings using ProkBERT,  focusing on the genomic features of ESKAPE pathogens with ProkBERT-mini.\n",
    "The workflow:\n",
    "1. **Model Loading**\n",
    "2. **Dataset Preparation**\n",
    "3. **Model Evaluation**\n",
    "4. **Results Visualization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6100a54-36aa-4ba8-ac84-c6eb37256de6",
   "metadata": {},
   "source": [
    "### Setup and Installation\n",
    "\n",
    "Before we start, let's ensure that all necessary libraries are installed for our project. This notebook uses packages, including `umap-learn` for dimensionality reduction and `seaborn` for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab99a9-5b88-47bb-9f44-74d8d8ca9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# umap, seaborn, HF datasets\n",
    "!pip install umap-learn seaborn datasets\n",
    "\n",
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664d9ed-d2b3-4009-aad1-3ff0c0492054",
   "metadata": {},
   "source": [
    "## Enabling and testing the GPU (if you are using google colab)\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d327ba7-bb1e-4bec-8e35-2add64c3a261",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "In this step, we'll utilize the MINI pretrained model of ProkBERT, focusing on the base model to extract sequence embeddings. It's important to match the model with the appropriate tokenizer, especially when loading directly from Hugging Face to ensure compatibility with tokenization parameters.\n",
    "\n",
    "**Embeddings:**\n",
    "\n",
    "Embeddings are dense vector representations of data, in this case, genomic sequences, where similar sequences are closer in the vector space. This representation allows the model to capture the context and semantic meanings of sequences, facilitating more effective analysis and comparison. By extracting embeddings from the ProkBERT model, we can leverage these rich, contextually informed representations for various bioinformatics applications, such as clustering, similarity searches, or as features for downstream machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ef6d4-b476-433e-8df5-b6a89f5acbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the prokbert-mini model\n",
    "model_name_path = 'neuralbioinfo/prokbert-mini-long'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_path, trust_remote_code=True)\n",
    "# We are going to use base, encoder model\n",
    "model = AutoModel.from_pretrained(model_name_path, trust_remote_code=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0549de3-cdc3-402b-8e00-051bc4475c50",
   "metadata": {},
   "source": [
    "### Dataset Preparation and Tokenization\n",
    "\n",
    "This section demonstrates preparing a dataset for tokenization and model training. A subset of 1000 samples is selected from the Hugging Face dataset for quick prototyping. The dataset is shuffled to ensure randomness, and tokenization is applied using the ProkBERT tokenizer. \n",
    "\n",
    "Sequences are padded, truncated to a maximum length of 512 tokens, and processed efficiently using multiprocessing. This setup ensures the data is ready for use with the ProkBERT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b012ab-582f-4ed4-81f6-144f44af7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"neuralbioinfo/ESKAPE-genomic-features\", split='ESKAPE')\n",
    "dataset.shuffle()\n",
    "dataset_sample = dataset.select(range(1000))\n",
    "\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"segment\"],  # Replace 'sequence' with the actual column name if different\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,  # Set the maximum sequence length if needed\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset_sample.map(tokenize_function, batched=True, num_proc=num_cores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03167c3-2942-4640-a57f-9ece99901a03",
   "metadata": {},
   "source": [
    "### Generating Sequence Representations\n",
    "\n",
    "In this section, we use the Trainer API to compute sequence embeddings efficiently. The dataset is processed through the model to extract the last hidden states of the final layer. These hidden states are aggregated using a mean pooling operation across the sequence length dimension, resulting in a single vector representation for each sequence.\n",
    "\n",
    "The `TrainingArguments` define the evaluation settings, including batch size and output directories, while the `Trainer` simplifies the prediction process. This streamlined approach replaces manual batching and ensures compatibility with the dataset and model. The resulting representations can be used for downstream tasks like classification or visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd937aa3-18df-4ae2-8ca9-2226be4eda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    remove_unused_columns=True,  # Ensure compatibility with input format\n",
    "    logging_dir=\"./logs\",  # Logging directory\n",
    "    report_to=\"none\",  # No reporting needed\n",
    ")\n",
    "\n",
    "# Set up the Trainer for prediction and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,  # Dummy model\n",
    "    args=training_args,  # Evaluation arguments\n",
    ")\n",
    "predictions = trainer.predict(tokenized_dataset)\n",
    "last_hidden_states = predictions.predictions[0]\n",
    "representations = last_hidden_states.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b60f1e-e418-49ef-b2c4-208fca5fb8d4",
   "metadata": {},
   "source": [
    "### Visualizing Sequence Embeddings with UMAP\n",
    "\n",
    "This section demonstrates visualizing high-dimensional sequence embeddings using Uniform Manifold Approximation and Projection (UMAP). UMAP reduces the dimensionality of the embeddings to 2D while preserving their structural relationships, making it easier to interpret patterns and clusters in the data.\n",
    "\n",
    "**UMAP Parameters:**\n",
    "- **`n_neighbors`**: Determines the balance between local and global data structure. Higher values prioritize global structure.\n",
    "- **`min_dist`**: Controls the minimum spacing between points in the 2D space. Smaller values emphasize local details.\n",
    "- **`random_state`**: Ensures reproducibility of the visualization.\n",
    "\n",
    "After dimensionality reduction, the UMAP embeddings are added to a DataFrame for visualization. We use Seaborn's `FacetGrid` to create scatterplots categorized by features such as `strand` and `class_label`. This allows us to explore how the embeddings cluster based on these features, revealing potential patterns and relationships within the dataset.\n",
    "\n",
    "The visualization process provides an intuitive understanding of the model's learned representations and their alignment with biological features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609836ef-40ab-4c65-9158-55a7c4952bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions.last_hidden_state\n",
    "umap_random_state = 42\n",
    "n_neighbors=20\n",
    "min_dist = 0.4\n",
    "reducer = umap.UMAP(random_state=umap_random_state, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "print('Running UMAP ....')\n",
    "umap_embeddings = reducer.fit_transform(representations)\n",
    "\n",
    "dataset_df = dataset_sample.to_pandas()\n",
    "dataset_df['umap_1'] = umap_embeddings[:, 0]\n",
    "dataset_df['umap_2'] = umap_embeddings[:, 1]\n",
    "\n",
    "g = sns.FacetGrid(dataset_df, col=\"strand\", hue=\"class_label\", palette=\"Set1\", height=6)\n",
    "# Apply a scatterplot to each subplot\n",
    "g.map(sns.scatterplot, \"umap_1\", \"umap_2\")\n",
    "# Add a legend\n",
    "g.add_legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
