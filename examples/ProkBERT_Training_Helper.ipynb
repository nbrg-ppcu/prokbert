{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG59DW3yUyVJ"
   },
   "source": [
    "# Training Helper Utilities\n",
    "To ease the process of training and evaluation multiple models, we have implemented two helper classes `TrainingHelperM` and `TrainingHelperD`, alongside a few helper functions like `get_finetuned_model_name`. These tools help us keep a tidy and safe code.\n",
    "\n",
    "## `TrainingHelperM`\n",
    "This class stores metadata regarding the model we are training. Its attributes are the following:\n",
    "- `base_model_name`: This is only the name of the model without prefixes.\n",
    "- `batch_size`: Training batch size. This is auto inferred for known model (see below at `__init__()`).\n",
    "- `dataset_name`: Name of teh training dataset.\n",
    "- `epochs`: Number of training epochs.\n",
    "- `eval_split_name`: Name of the evaluation split of the dataset. Auto generated based on sequence length.\n",
    "- `gradient_accumulation_steps`: Gradient accumulation steps.\n",
    "- `huggingface_model_name`: This is the Hugging Face identifier of the base model we are using.\n",
    "- `huggingface_prefix`: The prefix string. Name of the developer team.\n",
    "- `learning_rate`: Learning rate used to train the model.\n",
    "- `seq_len`: Max length of the training sequences.\n",
    "- `separator`: Small string used to separate fields in the finetuned name.\n",
    "- `task`: Training task name.\n",
    "- `test_split_name`: Name of the testing split of the dataset. Auto generated too.\n",
    "- `train_split_name`: Name of the training split of the dataset. Auto generated too.\n",
    "\n",
    "### Methods:\n",
    "`TrainingHelperM` has a few methods to further help the programmer:\n",
    "- `from_json(path)`: Loads the model from a JSON file (more below).\n",
    "- `get_default_model()`: This method returns a Hugging Face AutoModelForSequenceClassification instantiated from the name stored in the helper.\n",
    "- `get_finetuned_model_name`: Returns the finetuned model name string.\n",
    "- `get_tokenizer()`: Returns the matching tokenizer to the model.\n",
    "- `get_tokenizer_function()`: Returns a tokenizer function which can be used by `dataset.map()` for pre-tokenization and other tasks.\n",
    "- `get_training_params()`: Returns a dictionary with the following keys: `batch_size`, `gradient_accumulation_steps`, `seq_len`, `learning_rate`, `epochs`.\n",
    "- `initialize_from_environment()`: Instantiates the model helper from environmental variables (more below).\n",
    "- `initialize_from_finetuned_name(name)`: Instantiates the model helper from a finetuned name (more below).\n",
    "- `to_json(path)`: The helper can save its metadata to a JSON file.\n",
    "\n",
    "### Initialization of the class\n",
    "There are four ways to initialize a `TrainingHelperM` class. First off we can do so by calling it's `__init__()` method. Secondly we can use the `initialize_from_environment` class method, to pull the metadata from environment variables. This is extremely helpful for SLURM array jobs, as one can start up a wide range of model trainings from a single launch script. Thirdly we can initialize the model from a JSON file created by the `to_json` method. Lastly we can use the `parse_model_helper_from_finetuned_name` factory function. This one takes a pretrained model name, created by the `get_finetuned_model_name(TrainingHelperD, TrainingHelperM)` function and returns with a `TrainingHelperM` object.\n",
    "\n",
    "- The `__init__(**kwargs)` method takes the following arguments:\n",
    "  - huggingface_model_name: (str) Required.\n",
    "  - epochs: (int) Optional. Defaults to 1.0\n",
    "  - learning_rate (float) optional. Defaults to 0.001.\n",
    "  - seq_len (int) Optional. Defaults to 512.\n",
    "  - batch_size (int) Optional. If not given it's inferred for known models, otherwise an error is thrown.\n",
    "  - gradient_accumulation_steps (int) Optional. If not given it's inferred for known models, otherwise an error is thrown.\n",
    "  Since many of the parameters are default or auto inferred those values might not be correct in every case! It is better practice to pass all known information to the `__init__` !\n",
    "\n",
    "- `initialize_from_environment()`: This is a class method factory function. It looks for the following environmental variables:\n",
    "  - `MODEL_NAME`: The full Hugging Face name of the model.\n",
    "  - `LEARNING_RATE`: The learning rate.\n",
    "  - `LS`: The maximal sequence length for the given training. (The model itself might be able to handle more!)\n",
    "  - `NUM_TRAIN_EPOCHS`: Number of training epochs.\n",
    "- `from_json(path)`: This class method loads the class from a JSON file.\n",
    "- `parse_model_helper_from_finetuned_name(name)`: Not recommended, as  it's unsafe. It only works for known models, as batch size and gradient accumulation steps are not part of the finetuned name, so they have to be auto inferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_63vK3GQhZJt",
    "outputId": "51d4a325-f9fe-4254-e613-b8ccd99fb1cd"
   },
   "source": [
    "# This install method is guaranteed to work in google colab, so it is preferred for this example. For more details please check the Readme\n",
    "#!git clone --single-branch --branch TrainHelper https://github.com/nbrg-ppcu/prokbert.git\n",
    "#%pip install ./prokbert -q;"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If you are testing from the repo itself\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.prokbert.traininghelper_utils import TrainingHelperM"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x00eH-sS8zve",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "outputId": "cb93919f-5ae7-4ae3-9161-8ba0c92e4ab0"
   },
   "source": "#from prokbert.traininghelper_utils import TrainingHelperM",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "URDUY1Gy9_YA"
   },
   "source": [
    "# Let's start by initializing the helpers through their __init__ methods\n",
    "\n",
    "model_helper = TrainingHelperM(\n",
    "    huggingface_model_name='neuralbioinfo/prokbert-mini-long')\n",
    "# Notice that no batch size or gradient accumulation steps is given. These are auto inferred since prokbert-mini-long is a known model\n",
    "# These batch sizes are calculated assuming 40GB NVIDIA A100-s\n",
    "del model_helper\n",
    "\n",
    "# To fully control parameters we can pass in everything through the arguments to init\n",
    "model_helper = TrainingHelperM(\n",
    "    huggingface_model_name='neuralbioinfo/prokbert-mini-long',\n",
    "    dataset_name='TEST',\n",
    "    epochs=1,\n",
    "    learning_rate=0.001,\n",
    "    seq_len=512,\n",
    "    batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    separator='___',\n",
    "    task='testingtask'\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ycth0R1OAMY9"
   },
   "source": [
    "# To initialize the helpers from environmental variables we need to set them up first\n",
    "import os\n",
    "os.environ['MODEL_NAME'] = 'neuralbioinfo/prokbert-mini-long'\n",
    "os.environ['LEARNING_RATE'] = '0.001'\n",
    "os.environ['LS'] = '256'\n",
    "os.environ['NUM_TRAIN_EPOCHS'] = '1'\n",
    "os.environ['TASK'] = 'phage-lifestyle'\n",
    "os.environ['DATASET_NAME'] = 'testdataset'\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# This will work because prokbert-mini-long is a known model and BS and GAC are auto inferred\n",
    "del model_helper\n",
    "model_helper = TrainingHelperM.initialize_from_environment()"
   ],
   "metadata": {
    "id": "Ca8MNwdc8Kum"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "del model_helper\n",
    "os.environ['MODEL_NAME'] = 'some_developer/some_model'\n",
    "# This fails because it is not a known model\n",
    "model_helper = TrainingHelperM.initialize_from_environment()"
   ],
   "metadata": {
    "id": "c7dCVyXD8TSk"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# However if we add BS and GAC to the environmental variables the initialization will work\n",
    "os.environ['BATCH_SIZE'] = '64'\n",
    "os.environ['GRADIENT_ACCUMULATION_STEPS'] = '4'\n",
    "model_helper = TrainingHelperM.initialize_from_environment()"
   ],
   "metadata": {
    "id": "LCpLxBUa8T80"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lets save the helper so we can try loading it back\n",
    "model_helper.to_json('model_helper.json')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the helper from JSON\n",
    "helper_two = TrainingHelperM.from_json('model_helper.json')\n",
    "print(\"The helper loaded back from JSON is equal to the previous\", helper_two == model_helper)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Now that we know how to initialize and save the helper let's look at their usage\n",
    "Both classes are decorated with `@dataclass` so they have equivalence and ordering operators (==, <, >, <=, >=). Also since they are dataclasses they can be printed out directly. Also they can be converted to dictionaries directly using the `asdict()` method from dataclasses."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's try printing\n",
    "print(model_helper)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Something looks off with those prints?\n",
    "As you might notice when printed like this the properties have a `_` prefix, and they are different, than described before. This is because these are the hidden internal values that are not supposed to be accessed or changed directly. The public properties of the models are all accessible through getter and setter methods, using the `@property` decorator.\n",
    "\n",
    "### So what can we access? These public properties\n",
    "Again the public properties of the model helper are the following:\n",
    "- `base_model_name`: This is only the name of the model without prefixes.\n",
    "- `batch_size`: Training batch size. This is auto inferred for known model (see below at `__init__()`).\n",
    "- `dataset_name`: Name of the training dataset.\n",
    "- `epochs`: Number of training epochs.\n",
    "- `eval_split_name`: Name of the evaluation split of the dataset. Auto generated based on sequence length.\n",
    "- `gradient_accumulation_steps`: Gradient accumulation steps.\n",
    "- `huggingface_model_name`: This is the Hugging Face identifier of the base model we are using.\n",
    "- `huggingface_prefix`: The prefix string. Name of the developer team.\n",
    "- `learning_rate`: Learning rate used to train the model.\n",
    "- `seq_len`: Max length of the training sequences.\n",
    "- `separator`: Small string used to separate fields in the finetuned name.\n",
    "- `task`: Training task name.\n",
    "- `test_split_name`: Name of the testing split of the dataset. Auto generated too.\n",
    "- `train_split_name`: Name of the training split of the dataset. Auto generated too.\n",
    "\n",
    "A few things to note here. The names of the dataset splits are generated from the sequence length used. As a result these are non-modifiable. Trying to do so will raise an error. Consequently, the dataset paths are non-modifiable too!\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Here we will access all the fields to see what's up\n",
    "# Model name and parameters\n",
    "print(\"Name of the Hugging Face model: \", model_helper.huggingface_model_name)\n",
    "# We can access the prefix and the basename separately\n",
    "print(\"Hugging Face prefix: \", model_helper.huggingface_prefix)\n",
    "print(\"Hugging Face base model name: \", model_helper.base_model_name)\n",
    "\n",
    "# Training parameters\n",
    "print(\"Number of training epochs: \", model_helper.epochs)\n",
    "print(\"Training batch size: \", model_helper.batch_size)\n",
    "print(\"Gradient accumulation steps: \", model_helper.gradient_accumulation_steps)\n",
    "print(\"Learning rate: \", model_helper.learning_rate)\n",
    "print(\"Sequence length: \", model_helper.seq_len)\n",
    "\n",
    "# Dataset parameters\n",
    "print(\"Name of the training dataset: \", model_helper.dataset_name)\n",
    "print(\"Name of the training split: \", model_helper.train_split_name)\n",
    "print(\"Name of the testing split: \", model_helper.test_split_name)\n",
    "print(\"Name of the evaluation split: \", model_helper.eval_split_name)\n",
    "\n",
    "# Other task specific parameters\n",
    "print(\"Name of the training task: \", model_helper.task)\n",
    "print(\"Substring to separate fields in the finetuned name: \", model_helper.separator)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Off to the more intriguing functionalities\n",
    "Here we will showcase the helper functions of the `TrainingHelperM` class. Namely: `TrainingHelperM.get_default_model()`, `TrainingHelperM.get_tokenizer()`, `TrainingHelperM.get_tokenizer_function()` and `TrainingHelperM.get_finetuned_model_name`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# These are more heavyweight operations it is recommended not to try these without a GPU\n",
    "\n",
    "model_helper = TrainingHelperM('neuralbioinfo/prokbert-mini-long') # Full default helper for prokbert\n",
    "\n",
    "model = model_helper.get_default_model() # This will return a Hugging Face AutoModelForClassification\n",
    "tokenizer = model_helper.get_tokenizer() # This returns the corresponding tokenizer\n",
    "tokenize_fn = model_helper.get_tokenizer_function() # Tokenizer function to use with dataset.map() for example"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's say we successfully trained out model and would like to save it\n",
    "finetuned_name = model_helper.get_finetuned_model_name()\n",
    "print(finetuned_name)\n",
    "#model.save_pretrained(finetuned_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# And later on one can initialize the helper from the finetuned name\n",
    "helper_two = TrainingHelperM.initialize_from_finetuned_name(finetuned_name, separator='___') # Separator can be set to something else"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Lastly sometimes we only need the training parameters\n",
    "These can be accessed by calling the `get_training_parameters` function like this:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_helper.get_training_params()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
