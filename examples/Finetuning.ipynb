{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a8f50f-4e73-4bcf-abe9-51eccc6a5f0a",
   "metadata": {},
   "source": [
    "# Fine-tuning with the ProkBERT Model Family\n",
    "This notebook demonstrates how to utilize ProkBERT's pre-trained models for transfer learning tasks. We will apply the model to identify promoter sequences, framed as a binary classification problem where each segment is assigned a label.\n",
    "\n",
    "The main steps include:\n",
    "- Preparing the dataset to outline the labels for each segment.\n",
    "- Tokenizing nucleotide sequences.\n",
    "- Configuring training parameters such as learning rate, epochs, batch size, etc.\n",
    "- Training and evaluating the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ef551-0106-4329-85c3-82c3b47d2ebe",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "While ProkBERT can operate on CPUs, leveraging GPUs significantly accelerates the process. Google Colab offers free GPU usage (subject to time and memory limits), making it an ideal platform for trying and experimenting with ProkBERT models.\n",
    "\n",
    "## Enabling and testing the GPU (if you are using google colab)\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "\n",
    "First, we'll install the ProkBERT package directly from its GitHub repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ff812-e2c5-494d-8066-3d996d0e5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProkBERT\n",
    "!pip install datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, roc_auc_score, recall_score\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1b7e0-d7d0-4cc5-8ae8-feb356b64812",
   "metadata": {},
   "source": [
    "Next, we'll confirm that we can connect to the GPU with pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5f172-42fe-45d4-ab19-e8d194aaca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError('GPU device not found')\n",
    "else:\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f'Found GPU at: {device_name}')\n",
    "num_cores = os.cpu_count() \n",
    "print(f'Number of available CPU cores: {num_cores}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86dcee-c5c3-44d4-94c8-5a3b5c08e43f",
   "metadata": {},
   "source": [
    "## Loading the Pretrained Model and Tokenizer for Transfer Learning\n",
    "\n",
    "In this section, we load the ProkBERT model and its tokenizer from Hugging Face to perform transfer learning for sequence classification tasks. Transfer learning allows us to leverage the knowledge captured during pretraining on large genomic datasets and fine-tune the model for specific downstream tasks, such as promoter sequence classification.\n",
    "\n",
    "### Model Overview\n",
    "\n",
    "The `ProkBertForSequenceClassification` class is a specialized architecture for sequence classification tasks. It extends the `ProkBertPreTrainedModel` by adding a custom classification head on top of the ProkBERT base model. Key features include:\n",
    "\n",
    "- **Weighted Sum Pooling**: Instead of relying solely on the `[CLS]` token, the model computes a weighted sum of hidden states across all sequence positions. These weights are learned through a linear layer and normalized using a softmax function. This approach captures contributions from all positions, ensuring more nuanced representation of sequence information.\n",
    "  \n",
    " \n",
    "- **Classification Head**: A fully connected layer maps the pooled output to logits, corresponding to the number of labels in the task.\n",
    "\n",
    "- **Loss Computation**: For binary classification, the model computes the Cross-Entropy loss based on the logits and ground truth labels.\n",
    "\n",
    "### Tokenizer Alignment\n",
    "\n",
    "The tokenizer, which uses Local Context Aware (LCA) tokenization, processes the nucleotide sequences into input tokens for the model. It is crucial that the tokenizer parameters, such as `k-mer` size and `shift`, align with the settings used during the model's pretraining. For this example, we use the `neuralbioinfo/prokbert-mini` model with its pre-configured tokenizer.\n",
    "\n",
    "### Loading the Model\n",
    "\n",
    "We use the `AutoTokenizer` and `AutoModelForSequenceClassification` classes to load the tokenizer and model from Hugging Face. This ensures compatibility with the model's architecture and pretrained weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5ef95-d99d-4f12-be01-bd2bb5786200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_path = 'neuralbioinfo/prokbert-mini'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_path, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_path, trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41118b2c-376a-425d-b187-d6c768340420",
   "metadata": {},
   "source": [
    "## Sequence Data Preparation\n",
    "\n",
    "This project focuses on prokaryotic promoter sequences, labeled with `y=1` for known promoters and `y=0` otherwise. Each sequence includes an 80bp region with the Transcription Start Site (TSS) located at position 60. \n",
    "\n",
    "The dataset is preprocessed to ensure clean nucleotide sequences without empty or invalid entries. For more details on preprocessing, refer to the [segmentation notebook](https://github.com/nbrg-ppcu/prokbert/blob/main/examples/Segmentation.ipynb).\n",
    "\n",
    "## Tokenizing and Preparing the Dataset\n",
    "\n",
    "This block handles the preparation of prokaryotic promoter sequences for model training by tokenizing, masking, and labeling the data.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - Sequences are tokenized using `batch_encode_plus` with padding and special tokens added.\n",
    "\n",
    "2. **Label Preparation**:\n",
    "   - The `y` column from the dataset is converted into a PyTorch tensor labeled as `labels`, which are included in the output dictionary.\n",
    "\n",
    "3. **Dataset Loading and Preprocessing**:\n",
    "   - The dataset is loaded from Hugging Face (`neuralbioinfo/bacterial_promoters`).\n",
    "   - It is randomized using a fixed seed (`42`) for reproducibility.\n",
    "\n",
    "4. **Tokenization Mapping**:\n",
    "   - The `tokenize_function` is applied across the dataset in batches using multiple CPU cores for efficiency.\n",
    "\n",
    "This prepares the dataset for training with ProkBERT, ensuring clean and compatible inputs for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce37821-7010-487f-af4a-c4f82bcd49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the input sequences\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        examples[\"segment\"],\n",
    "        padding=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Get the input_ids and attention_mask\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "    attention_mask = encoded[\"attention_mask\"]\n",
    "    \n",
    "    # Mask tokens with IDs 2 and 3 in a vectorized way\n",
    "    mask_tokens = (input_ids == 2) | (input_ids == 3)  # Identify where tokens 2 and 3 occur\n",
    "    attention_mask[mask_tokens] = 0  # Set attention_mask to 0 for these tokens\n",
    "    y = torch.tensor(examples[\"y\"], dtype=torch.int64)    \n",
    "    \n",
    "    # Return the updated dictionary\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": y,  # Include labels for training\n",
    "    }\n",
    "dataset = load_dataset(\"neuralbioinfo/bacterial_promoters\")\n",
    "randomize_dataset = dataset.shuffle(seed=42)\n",
    "randomize_dataset.flatten_indices()\n",
    "tokenized_dataset = randomize_dataset.map(tokenize_function, batched=True, num_proc=num_cores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e2a65-8b2c-4950-abf1-0e71ab337dde",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "For promoter prediction, the following metrics are computed to evaluate model performance:\n",
    "\n",
    "- **MCC (Matthews Correlation Coefficient)**: A balanced metric that considers true and false positives and negatives. \n",
    "\n",
    "- **Accuracy**: The proportion of correct predictions among all samples, providing an overall sense of the model's performance.\n",
    "\n",
    "- **Recall**: Measures the ability of the model to identify all positive samples (true promoters). It is weighted to account for class imbalances.\n",
    "\n",
    "- **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**: Evaluates the model's capability to distinguish between classes. For binary classification, it focuses on the positive class probabilities; for multiclass classification, it uses a one-vs-rest approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca54586-fe13-405a-a91d-13b83ec31753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)  # Predicted class indices\n",
    "\n",
    "    # Calculate metrics\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "\n",
    "    try:\n",
    "        # Adjust for binary or multiclass classification\n",
    "        if logits.shape[1] == 2:  # Binary classification\n",
    "            roc_auc = roc_auc_score(labels, logits[:, 1])  # Use positive class probabilities\n",
    "        else:  # Multiclass classification\n",
    "            roc_auc = roc_auc_score(labels, logits, multi_class='ovr')  # Use probabilities for all classes\n",
    "    except ValueError:\n",
    "        # Handle edge cases where ROC-AUC cannot be computed\n",
    "        roc_auc = float('nan')\n",
    "\n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"accuracy\": acc,\n",
    "        \"recall\": recall,\n",
    "        \"roc_auc\": roc_auc,\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbcaf3-61c9-4086-a2d6-853377094686",
   "metadata": {},
   "source": [
    "### Training the ProkBERT Model\n",
    "\n",
    "We fine-tune the ProkBERT model for promoter prediction using a customized training setup. The key steps include configuring training parameters, initializing the Trainer, and starting the training process.\n",
    "\n",
    "#### Training Configuration\n",
    "The `TrainingArguments` define the training and evaluation setup:\n",
    "- **Batch Size**: Both training and evaluation use a batch size of 128 for efficient processing.\n",
    "- **Evaluation Strategy**: Evaluations are conducted every 50 steps to monitor progress.\n",
    "- **Logging**: Training metrics are logged every 50 steps, and the logs are stored in the `./logs` directory.\n",
    "- **Learning Rate**: A learning rate of `0.0004` is used for gradient updates.\n",
    "- **Epochs**: The model is trained for 1 epoch, suitable for demonstration or initial fine-tuning.\n",
    "- **Checkpointing**: Only the most recent checkpoint is saved to conserve storage.\n",
    "\n",
    "#### Trainer Initialization\n",
    "The Hugging Face `Trainer` API simplifies model training by:\n",
    "- Automatically handling batching, gradient updates, and evaluation.\n",
    "- Using `compute_metrics` to evaluate the model's performance on the test dataset (`test_sigma70`).\n",
    "\n",
    "#### Training Process\n",
    "The `trainer.train()` method starts the training loop, fine-tuning the ProkBERT model on the provided training dataset. Progress is logged, and metrics are computed periodically to assess the model's learning.\n",
    "\n",
    "This setup efficiently adapts the pre-trained ProkBERT model to the promoter prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759e782-d366-4f8d-97ea-9c94fb27f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=128,\n",
    "    per_device_train_batch_size=128,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=0.0004,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test_sigma70'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb5ada-6801-40ed-b79c-a07d50e0a139",
   "metadata": {},
   "source": [
    "# Fine-tuned Model\n",
    "\n",
    "The final fine-tuned model is available at the specified path, ready for deployment or further evaluation. While the current setup provides a good foundation, there's always room for improvement by experimenting with different hyperparameters. Fine-tuning these parameters can help greatly improve the model's performance on specific tasks or datasets.\n",
    "\n",
    "## Considerations for Further Optimization\n",
    "\n",
    "- **Experiment with Hyperparameters**: Adjust learning rate, batch size, number of epochs, and other training parameters to find the optimal configuration for your specific use case.\n",
    "- **Cross-validation**: Use cross-validation techniques to ensure that your model generalizes well across different subsets of your data.\n",
    "- **Data Augmentation**: Explore data augmentation strategies for sequence data, such as introducing random mutations or utilizing synthetic data generation, to increase the robustness of your model.\n",
    "- **Advanced Architectures**: Consider experimenting with different model architectures or integrating additional layers (i.e. convolution could be a good idea) to improve the model's capacity to capture complex patterns in the data.\n",
    "\n",
    "## Closing Remarks\n",
    "\n",
    "Fine-tuning a pre-trained model like ProkBERT offers a powerful approach to leveraging large language moels for biological sequence analysis. By carefully selecting and optimizing your model's hyperparameters, you can achieve significant improvements in performance. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
