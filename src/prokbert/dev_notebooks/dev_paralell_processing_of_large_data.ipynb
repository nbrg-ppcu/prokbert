{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b35cc9-703e-425d-97a1-8e4cc00f7a65",
   "metadata": {},
   "source": [
    "## Dataset létrehozása fasta file-okból indulva.\n",
    "Input: folder, amiben a fasta file-ok vannak.\n",
    "Output: X mátrix, amiben a tokenizált vektorok vannak. \n",
    "Opcinálisan egy adatbázis a szegmensekről, gyakorlatban a szekvencia adatot nem fogjuk tárolni, csak a koordinátákat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d105b140-073d-46e3-bec5-b07711bf4c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 08:00:49,942 - INFO - Loading sequence data into memory!\n",
      "2023-08-11 08:00:49,958 - INFO - Checking input DataFrame!\n",
      "2023-08-11 08:00:49,958 - INFO - Checking input sequence_id is valid primary key in the DataFrame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: /home/ligeti/github/prokbert/src/prokbert/configs/sequence_processing.yaml\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "/home/ligeti/github/prokbert/src/prokbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 08:00:50,141 - INFO - Tokenization of a list of segments\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "import pathlib\n",
    "from os.path import join\n",
    "import os\n",
    "import sys\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "current_path = str(pathlib.Path(os.getcwd()).parent)\n",
    "sys.path.append(current_path)\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('../)\n",
    "\n",
    "from config_utils import *\n",
    "from sequtils import *\n",
    "\n",
    "    \n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "#pd.set_option('display.width', 4000)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "defconfig = SeqConfig()\n",
    "         \n",
    "segmentation_params = defconfig.get_set_segmentation_parameters()\n",
    "tokenization_params = defconfig.get_and_set_tokenization_params({'shift':2})             \n",
    "comp_params = defconfig.get_set_computational_paramters()\n",
    "\n",
    "num_cores = comp_params['cpu_cores_for_tokenization']\n",
    "batch_size = comp_params['batch_size_tokenization']\n",
    "numpy_dtype=comp_params['np_tokentype'] # Note that if you use kmer>7 then other is recommended\n",
    "           \n",
    "input_fasta_dir = join(current_path, 'data/sample_data/pretraining')\n",
    "\n",
    "input_fasta_files = [join(input_fasta_dir, file) for file in get_non_empty_files(input_fasta_dir)]\n",
    "contigs = load_contigs(input_fasta_files,IsAddHeader=True,AsDataFrame=True, adding_reverse_complement=False)\n",
    "contigs['sequence_id'] = list(range(len(contigs)))\n",
    "contigs = contigs[['sequence', 'sequence_id']]\n",
    "segment_db = segment_sequences(contigs, segmentation_params, AsDataFrame=True)\n",
    "\n",
    "tokenized = batch_tokenize_segments_with_ids(segment_db, tokenization_params, num_cores, batch_size, numpy_dtype)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7cc6695-b291-427b-b2c7-14d799d0e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 08:00:52,488 - INFO - Doing randomization!\n",
      "2023-08-11 08:00:52,491 - INFO - Tuncating all zeros column\n"
     ]
    }
   ],
   "source": [
    "shift = tokenization_params['shift']\n",
    "expected_max_token = max(len(arr) for arrays in tokenized.values() for arr in arrays)\n",
    "X, torchdb = get_rectangular_array_from_tokenized_dataset(tokenized,\n",
    "                                                          tokenization_params['shift'],\n",
    "                                                          expected_max_token, numpy_dtype = numpy_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70dbdf4f-cfb5-41a1-aad9-0626988463cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ATCGCT', 'CGCTAT', 'CTATGG'], ['TCGCTA', 'GCTATG', 'TATGGT']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[2, 876, 1656, 1855, 3], [2, 3489, 2515, 3312, 3]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### act_segment = 'ATCGCTATGGT'\n",
    "tokenized_seg, kmers = lca_tokenize_segment(act_segment, tokenization_params)\n",
    "kmers\n",
    "tokenized_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa58fa2-edd2-4758-91f5-a1373bc31ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 08:10:04,048 - INFO - Nr. line to cover the seq:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ATCGCTATGGT\n",
      "0.  ATCGCT\n",
      "1.    CGCTAT\n",
      "2.      CTATGG\n",
      "3.        \n"
     ]
    }
   ],
   "source": [
    "lines = pretty_print_overlapping_sequence(act_segment, kmers[0], tokenization_params)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33823aaa-9958-40f8-a5eb-317d89b9a96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508958f2-c8ae-41bb-8f9b-1275ad607ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4811d-c638-499a-8995-76c45b2e4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034fb36-6601-4a7e-866b-95fdc6059f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
